{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0eb519",
   "metadata": {},
   "source": [
    "下载好模型\n",
    "pip install -U huggingface_hub\n",
    "\n",
    "\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "huggingface-cli download --resume-download openai/clip-vit-large-patch14-336 --local-dir openai/clip-vit-large-patch14-336 --local-dir-use-symlinks False\n",
    "\n",
    " \n",
    "huggingface-cli download --resume-download Qwen/Qwen1.5-4B-Chat --local-dir Qwen1.5-4B-Chat --local-dir-use-symlinks False \n",
    "0.5/1.8/4/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "modify_qwen_tokenizer_dir = \"Qwen1.5-4B-Chat\"\n",
    "modify_qwen_tokenizer = AutoTokenizer.from_pretrained(modify_qwen_tokenizer_dir)\n",
    "\n",
    "modify_qwen_tokenizer.encode(\"<image>\")\n",
    "\n",
    "#此处的占位符有三个维度，我们要修改到一个维度 [27, 1805, 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(modify_qwen_tokenizer)\n",
    "#151647"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0bf096",
   "metadata": {},
   "source": [
    "Q: 加了这个新token，需要修改模型的embedding模块么？ A：不需要，qwen_model.model.embed_tokens留了足够的空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model=AutoModelForCausalLM.from_pretrained(\n",
    "    modify_qwen_tokenizer_dir,device_map=\"cuda:1\",torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c32454",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model.model.embed_tokens\n",
    "#Embedding(151936, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenizer 负责把文字变成 ID\n",
    "text = \"你好\"\n",
    "input_ids = modify_qwen_tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "print(\"1. 文字:\", text)\n",
    "print(\"2. Tokenizer 翻译出的 ID:\", input_ids) \n",
    "# 结果可能像: tensor([[108386]])\n",
    "\n",
    "# 2. Embed_tokens 负责把 ID 变成向量\n",
    "# 注意：我们要把 ID 扔给 embed_tokens 层\n",
    "# (如果模型在 CPU，这里 input_ids 也在 CPU 就行)\n",
    "vectors = qwen_model.model.embed_tokens(input_ids)\n",
    "\n",
    "print(\"3. 最终查到的向量形状:\", vectors.shape)\n",
    "# 结果应该是: torch.Size([1, 1, 2048]) -> [Batch, 序列长度, 隐藏层维度]\n",
    "# 这说明\"你好\"这个词，被转换成了长度为 2048 的一串数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model.lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eedeba",
   "metadata": {},
   "source": [
    "embed_tokens：字→向量。让计算机读懂输入。  \n",
    "lm_head：向量->字的概率。让计算机生成输出。  \n",
    "它们就像的一座大桥的桥头和桥尾，中间的桥身就是那一堆 Transformer 层  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name_or_path = (\n",
    "    \"openai/clip-vit-large-patch14-336\"\n",
    ")\n",
    "qwen_model_name_or_path = \"Qwen1.5-4B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca255376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(clip_model_name_or_path, device_map=\"cuda:1\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_name_or_path, device_map=\"cuda:1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ebd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained(qwen_model_name_or_path)\n",
    "llm_tokenizer.encode(\"<image>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79b8c8",
   "metadata": {},
   "source": [
    "初始化llava\n",
    "将clip模型和llm_model模型的config拿出来，初始化一个llava model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd80d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a CLIP-vision config\n",
    "vision_config = clip_model.vision_model.config\n",
    "\n",
    "# Initializing a Llama config\n",
    "text_config = llm_model.config\n",
    "\n",
    "# Initializing a Llava llava-1.5-7b style configuration\n",
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "\n",
    "# Initializing a model from the llava-1.5-7b style configuration\n",
    "model = LlavaForConditionalGeneration(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa29f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_tower.vision_model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3421d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.vision_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284388a9",
   "metadata": {},
   "source": [
    "形状初始化好了，模型权重都还是随机生成的，需要把两个模型的权重，复制过去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16108591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_tower.vision_model = clip_model.vision_model\n",
    "model.language_model = llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b885c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把qwen tokenizer分词器里定义的填充 token 编号同步给模型配置\n",
    "model.config.pad_token_id = llm_tokenizer.pad_token_id\n",
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.image_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb136db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tokenizer.encode(\"<image>\")[0]#这个值需要被替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.image_token_index = llm_tokenizer.encode(\"<image>\")[0]\n",
    "model.config.image_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"show_model_v1/model001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tokenizer.save_pretrained(\"show_model_v1/model001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e799f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoprocessor = AutoProcessor.from_pretrained(clip_model_name_or_path)\n",
    "autoprocessor.save_pretrained(\"show_model_v1/model002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb082a",
   "metadata": {},
   "source": [
    "主要需要把show_model/model002里面的preprocessor_config.json文件，放在show_model/model001里面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a823c",
   "metadata": {},
   "source": [
    "重启内核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda4e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ZJH/miniconda3/envs/mote/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Qwen2Tokenizer'. \n",
      "The class this function is called from is 'CLIPTokenizerFast'.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model_name_or_path = \"show_model_v1/model001\"  # \n",
    "# model_name_or_path = \"test_model_copy/model001\"  #\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path, device_map=\"cuda:1\", torch_dtype=torch.bfloat16\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57c9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = (\n",
    "'<image>\\nRelay a brief, clear account of the picture shown.', # 提问\n",
    " 'a big mountain with a lake', # 真实答案\n",
    " 'R.jpeg' # 图片路径\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb2c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_input(model, processor, testdata:tuple):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": testdata[0]},\n",
    "    ]\n",
    "    prompt = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # print(prompt)\n",
    "    # print(\"*\"*20)\n",
    "    image = Image.open(testdata[2])\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    for tk in inputs.keys():\n",
    "        if tk == \"pixel_values\":\n",
    "            inputs[tk] = inputs[tk].to(model.device, dtype=torch.bfloat16)\n",
    "        else:\n",
    "            inputs[tk] = inputs[tk].to(model.device)\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    \n",
    "    generate_ids = [\n",
    "        oid[len(iids):] for oid, iids in zip(generate_ids, inputs.input_ids)\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    gen_text = processor.batch_decode(generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
    "    return gen_text,inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37e12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result, debug_inputs = build_model_input(model, processor, testdata)\n",
    "\n",
    "# 'the kitchen is a bright yellow with a glass top island and a large window that looks out to the'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842c5580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ' m   s o r r y ,   b u t   I   c a n n o t   p r o v i d e   a   p i c t u r e   a s   I   a m   a n   A I   l a n g u a g e   m o d e l   a n d   d o   n o t\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166f90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n",
      "torch.Size([1, 33])\n"
     ]
    }
   ],
   "source": [
    "# 查看 inputs 包含哪些键\n",
    "print(debug_inputs.keys())\n",
    "\n",
    "# 查看 input_ids 的形状\n",
    "print(debug_inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4735af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"show_model_v1/model001\",\n",
       "  \"architectures\": [\n",
       "    \"LlavaForConditionalGeneration\"\n",
       "  ],\n",
       "  \"ignore_index\": -100,\n",
       "  \"image_seq_length\": 576,\n",
       "  \"image_token_index\": 151646,\n",
       "  \"model_type\": \"llava\",\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"projector_hidden_act\": \"gelu\",\n",
       "  \"text_config\": {\n",
       "    \"_attn_implementation_autoset\": true,\n",
       "    \"_name_or_path\": \"Qwen1.5-4B-Chat\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"Qwen2ForCausalLM\"\n",
       "    ],\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 151643,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 151645,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"silu\",\n",
       "    \"hidden_size\": 2560,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 6912,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 32768,\n",
       "    \"max_window_layers\": 21,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"qwen2\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 20,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 40,\n",
       "    \"num_key_value_heads\": 20,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": null,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_scaling\": null,\n",
       "    \"rope_theta\": 5000000.0,\n",
       "    \"sep_token_id\": null,\n",
       "    \"sliding_window\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": false,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": \"bfloat16\",\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"use_sliding_window\": false,\n",
       "    \"vocab_size\": 151936\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"vision_config\": {\n",
       "    \"_attn_implementation_autoset\": true,\n",
       "    \"_name_or_path\": \"\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": null,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": null,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"dropout\": 0.0,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": null,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"quick_gelu\",\n",
       "    \"hidden_size\": 1024,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"image_size\": 336,\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_channels\": 3,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": null,\n",
       "    \"patch_size\": 14,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"projection_dim\": 768,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false\n",
       "  },\n",
       "  \"vision_feature_layer\": -2,\n",
       "  \"vision_feature_select_strategy\": \"default\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
