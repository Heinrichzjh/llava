{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ZJH/miniconda3/envs/mote/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "The tokenizer you are loading from '/home/ZJH/llava/onevision/llava_onevision' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: The image depicts a breathtaking mountainous landscape, likely situated in a high-altitude region. The central focus is a towering snow-capped peak that dominates the scene, its rugged surface covered with pristine white snow and ice. This peak appears to be part of a larger mountain range, as several other peaks can be seen in the background, each varying in height and shape.\n",
      "\n",
      "In the foreground, there is a serene lake reflecting the vibrant colors of the surrounding environment. The water is calm, creating a mirror-like surface that perfectly captures the reflection of the mountains, trees, and sky above. The lake's edge is lined with dense forests, showcasing a mix of evergreen and deciduous trees. The foliage displays a rich palette of autumnal hues, including shades of green, yellow, orange, and red, indicating that the season is fall.\n",
      "\n",
      "The terrain around the lake slopes gently downward from the forested area towards the base of the mountains. The lower slopes are covered with a variety of vegetation, including grasses and shrubs, which add to the diversity of the landscape. The rocky outcrops and cliffs visible on the sides of the mountains suggest a geologically active region, possibly prone to glacial movements or erosion.\n",
      "\n",
      "Above all, the sky is a clear, vivid blue, devoid of clouds, suggesting fair weather conditions. The lighting in the image indicates that it was taken during the day when the sun was at a high angle, casting bright light across the entire scene and enhancing the natural beauty of the landscape.\n",
      "\n",
      "This image encapsulates the grandeur and tranquility of a mountainous region during the fall season, highlighting the interplay between the elements of nature such as water, land, and sky. The combination of the reflective lake, colorful foliage, and majestic snow-capped peaks creates a harmonious and visually stunning composition.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 如果您之前已经下载到本地项目文件夹，请使用本地路径\n",
    "# model_path = \"lmms-lab/LLaVA-OneVision-1.5-4B-Instruct\" \n",
    "model_path = \"/home/ZJH/llava/onevision/llava_onevision_1\" # 假设这是您的本地路径\n",
    "\n",
    "# 1. 加载 Processor 和 Model\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"cuda:0\", \n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 2. 准备图像\n",
    "image_path = \"/home/ZJH/llava/build_llava/R.jpeg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# 3. 关键修改：使用标准的 Messages 格式，而不是手动拼接字符串\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},  # 这里只是占位，告诉模板这里有一张图\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 4. 使用 apply_chat_template 自动生成正确的 Prompt\n",
    "# 这会自动添加正确的 <|im_start|>, <|image_pad|> 等特殊 Token\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# 5. 处理输入\n",
    "inputs = processor(\n",
    "    images=image, \n",
    "    text=prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# 6. 推理\n",
    "output_ids = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# 7. 解码输出\n",
    "# 只需要解码新生成的 token\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"Model Response: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8d0fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[-1.7777, -1.7777, -1.7777,  ...,  2.0464,  2.0464,  2.0464],\n",
       "        [-1.7777, -1.7777, -1.7777,  ...,  2.0464,  2.0464,  2.0464],\n",
       "        [-1.7485, -1.7485, -1.7485,  ...,  2.0748,  2.0748,  2.0748],\n",
       "        ...,\n",
       "        [-0.0405, -0.0113,  0.4413,  ..., -1.3665, -1.4091, -1.4376],\n",
       "        [-0.0550,  0.0179,  0.1493,  ..., -1.2811, -1.1674, -1.1105],\n",
       "        [ 0.0033,  0.0325, -0.0842,  ..., -1.1389, -1.0821, -1.1532]],\n",
       "       device='cuda:0'), 'image_grid_thw': tensor([[  1,  66, 136]], device='cuda:0')})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c58f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2269])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab75ab12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2269])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfa80f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8976, 588])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e94a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.image_grid_thw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02dfcd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llavaonevision1_5Config {\n",
       "  \"architectures\": [\n",
       "    \"LLaVAOneVision1_5_ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"configuration_llavaonevision1_5.Llavaonevision1_5Config\",\n",
       "    \"AutoModel\": \"modeling_llavaonevision1_5.LLaVAOneVision1_5_ForConditionalGeneration\",\n",
       "    \"AutoModelForCausalLM\": \"modeling_llavaonevision1_5.LLaVAOneVision1_5_ForConditionalGeneration\"\n",
       "  },\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"image_token_id\": 151655,\n",
       "  \"model_type\": \"llavaonevision1_5\",\n",
       "  \"text_config\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"dtype\": \"bfloat16\",\n",
       "    \"head_dim\": 128,\n",
       "    \"hidden_act\": \"silu\",\n",
       "    \"hidden_size\": 2560,\n",
       "    \"image_token_id\": null,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 9728,\n",
       "    \"layer_types\": [\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\",\n",
       "      \"full_attention\"\n",
       "    ],\n",
       "    \"max_position_embeddings\": 262144,\n",
       "    \"max_window_layers\": 36,\n",
       "    \"model_type\": \"LLaVAOneVision1_5_text\",\n",
       "    \"num_attention_heads\": 32,\n",
       "    \"num_hidden_layers\": 36,\n",
       "    \"num_key_value_heads\": 8,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_scaling\": null,\n",
       "    \"rope_theta\": 5000000.0,\n",
       "    \"sliding_window\": null,\n",
       "    \"use_cache\": true,\n",
       "    \"use_sliding_window\": false,\n",
       "    \"video_token_id\": null,\n",
       "    \"vocab_size\": 151936\n",
       "  },\n",
       "  \"transformers_version\": \"4.57.3\",\n",
       "  \"video_token_id\": 151656,\n",
       "  \"vision_config\": {\n",
       "    \"depth\": 24,\n",
       "    \"dtype\": \"bfloat16\",\n",
       "    \"embed_dim\": 1024,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_size\": 1024,\n",
       "    \"in_channels\": 3,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"model_type\": \"rice_vit\",\n",
       "    \"num_heads\": 16,\n",
       "    \"patch_size\": 14,\n",
       "    \"spatial_merge_size\": 2,\n",
       "    \"temporal_patch_size\": 1,\n",
       "    \"text_hidden_size\": 2560\n",
       "  },\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd5c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from typing import Dict,List,Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4614e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive-action'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir=\"drive-action\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430dd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/home/ZJH/llava/onevision/drive-action/data/train-00000-of-00013.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d99406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      question_slice_id   qa_l0                qa_l1  \\\n",
      "0  d752a4f2-1f4b-11f0-b3a2-644ed76caecb  Vision  Navigation Position   \n",
      "1  db72cfb2-1f4b-11f0-b3a2-644ed76caecb  Vision  Navigation Position   \n",
      "2  d9789e8a-1f4b-11f0-b3a2-644ed76caecb  Vision  Navigation Position   \n",
      "3  d9789e8a-1f4b-11f0-b3a2-644ed76caecb  Vision  Navigation Position   \n",
      "4  db273d40-1f4b-11f0-b3a2-644ed76caecb  Vision  Navigation Position   \n",
      "\n",
      "      question_category                                         content_cn  \\\n",
      "0  true_false_questions  {'answer': '正确', 'question': '前方47米，导航指示向右前方行驶...   \n",
      "1  true_false_questions  {'answer': '正确', 'question': '前方17米，导航指示左转掉头，前...   \n",
      "2      choice_questions  {'answer': 'C', 'question': '前方14米，导航指示向右前方行驶，...   \n",
      "3  true_false_questions  {'answer': '正确', 'question': '前方14米，导航指示向右前方行驶...   \n",
      "4      choice_questions  {'answer': 'C', 'question': '前方102米，导航指示右转，前方2...   \n",
      "\n",
      "                                          content_en  \\\n",
      "0  {'answer': 'True', 'question': '47 meters ahea...   \n",
      "1  {'answer': 'True', 'question': '17 meters ahea...   \n",
      "2  {'answer': 'C', 'question': '14 meters ahead, ...   \n",
      "3  {'answer': 'True', 'question': '14 meters ahea...   \n",
      "4  {'answer': 'C', 'question': '102 meters ahead,...   \n",
      "\n",
      "                                             image_0  \\\n",
      "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "\n",
      "                                             image_1  \\\n",
      "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "\n",
      "                                             image_2  \n",
      "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n",
      "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n",
      "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n",
      "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n",
      "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())          # 打印前几行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d383906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question_slice_id', 'qa_l0', 'qa_l1', 'question_category',\n",
      "       'content_cn', 'content_en', 'image_0', 'image_1', 'image_2'],\n",
      "      dtype='object')\n",
      "1245\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)         # 查看字段\n",
    "print(len(df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56becc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['content_en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0be4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4c84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ZJH/miniconda3/envs/mote/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 如果您之前已经下载到本地项目文件夹，请使用本地路径\n",
    "# model_path = \"lmms-lab/LLaVA-OneVision-1.5-4B-Instruct\" \n",
    "model_path = \"/home/ZJH/llava/onevision/llava_onevision_1\" # 假设这是您的本地路径\n",
    "\n",
    "# 1. 加载 Processor 和 Model\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"cuda:0\", \n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17916fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaOnevisionModel(\n",
      "  (vision_tower): SiglipVisionModel(\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(729, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-25): 26 x SiglipEncoderLayer(\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (self_attn): SiglipAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): GELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): LlavaOnevisionMultiModalProjector(\n",
      "    (linear_1): Linear(in_features=1152, out_features=3584, bias=True)\n",
      "    (act): GELUActivation()\n",
      "    (linear_2): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "  )\n",
      "  (language_model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152128, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea3646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor type: <class 'transformers.models.llava_onevision.processing_llava_onevision.LlavaOnevisionProcessor'>\n"
     ]
    }
   ],
   "source": [
    "# 1) processor 类型\n",
    "from transformers import AutoProcessor\n",
    "proc = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "print(\"processor type:\", type(proc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d3a4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[14990]]), 'attention_mask': tensor([[1]]), 'pixel_values': tensor([[[[[-0.6078,  0.7333, -0.0431,  ...,  0.2549, -0.1922, -0.9216],\n",
       "           [ 0.0824, -0.2549, -0.9843,  ...,  0.8745,  0.1137,  0.3569],\n",
       "           [-0.9216,  0.6314,  0.8745,  ..., -0.6941,  0.8196,  0.3804],\n",
       "           ...,\n",
       "           [ 0.7098, -0.2549, -0.5529,  ..., -0.9216,  0.1765,  0.0745],\n",
       "           [-0.0039, -0.2157,  0.9922,  ...,  0.7490,  0.5686,  0.3333],\n",
       "           [-0.7569, -0.2784,  0.9765,  ...,  0.4196,  0.1373, -0.9765]],\n",
       "\n",
       "          [[-0.5529, -0.5529, -0.0118,  ...,  0.5843, -0.1137,  0.8039],\n",
       "           [-0.7255,  0.0588,  0.2549,  ..., -0.4980, -0.7255,  0.8824],\n",
       "           [-0.9686,  0.5373, -0.4980,  ..., -0.0196, -0.9922,  0.6078],\n",
       "           ...,\n",
       "           [ 0.0118, -0.2784,  0.3647,  ...,  0.6941, -0.6706, -0.4824],\n",
       "           [-0.0667,  0.3882, -0.4902,  ..., -0.8667,  0.0667, -0.9608],\n",
       "           [-0.3412, -0.1059, -0.7804,  ...,  0.3569,  0.2235, -0.9765]],\n",
       "\n",
       "          [[-0.8275,  0.6000, -0.6784,  ..., -0.6235, -0.8275,  0.5137],\n",
       "           [ 0.4431, -0.5137, -0.7255,  ..., -0.7098, -0.4510,  0.1529],\n",
       "           [-0.0275,  0.8588, -0.1529,  ...,  0.1686, -0.9294,  0.8431],\n",
       "           ...,\n",
       "           [-0.5216,  0.2392,  0.4275,  ..., -0.3176,  0.7490, -0.1529],\n",
       "           [-0.0118,  0.4275,  0.4980,  ..., -0.4431, -0.8196,  0.9843],\n",
       "           [ 0.4353, -0.4431,  0.3490,  ..., -0.5059,  0.8667,  0.4667]]],\n",
       "\n",
       "\n",
       "         [[[-0.6078,  0.7333, -0.0431,  ...,  0.2549, -0.1922, -0.9216],\n",
       "           [ 0.0824, -0.2549, -0.9843,  ...,  0.8745,  0.1137,  0.3569],\n",
       "           [-0.9216,  0.6314,  0.8745,  ..., -0.6941,  0.8196,  0.3804],\n",
       "           ...,\n",
       "           [ 0.7098, -0.2549, -0.5529,  ..., -0.9216,  0.1765,  0.0745],\n",
       "           [-0.0039, -0.2157,  0.9922,  ...,  0.7490,  0.5686,  0.3333],\n",
       "           [-0.7569, -0.2784,  0.9765,  ...,  0.4196,  0.1373, -0.9765]],\n",
       "\n",
       "          [[-0.5529, -0.5529, -0.0118,  ...,  0.5843, -0.1137,  0.8039],\n",
       "           [-0.7255,  0.0588,  0.2549,  ..., -0.4980, -0.7255,  0.8824],\n",
       "           [-0.9686,  0.5373, -0.4980,  ..., -0.0196, -0.9922,  0.6078],\n",
       "           ...,\n",
       "           [ 0.0118, -0.2784,  0.3647,  ...,  0.6941, -0.6706, -0.4824],\n",
       "           [-0.0667,  0.3882, -0.4902,  ..., -0.8667,  0.0667, -0.9608],\n",
       "           [-0.3412, -0.1059, -0.7804,  ...,  0.3569,  0.2235, -0.9765]],\n",
       "\n",
       "          [[-0.8275,  0.6000, -0.6784,  ..., -0.6235, -0.8275,  0.5137],\n",
       "           [ 0.4431, -0.5137, -0.7255,  ..., -0.7098, -0.4510,  0.1529],\n",
       "           [-0.0275,  0.8588, -0.1529,  ...,  0.1686, -0.9294,  0.8431],\n",
       "           ...,\n",
       "           [-0.5216,  0.2392,  0.4275,  ..., -0.3176,  0.7490, -0.1529],\n",
       "           [-0.0118,  0.4275,  0.4980,  ..., -0.4431, -0.8196,  0.9843],\n",
       "           [ 0.4353, -0.4431,  0.3490,  ..., -0.5059,  0.8667,  0.4667]]]]]), 'image_sizes': tensor([[384, 384]]), 'batch_num_images': tensor([1])})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = Image.fromarray((np.random.rand(384,384,3)*255).astype(\"uint8\"))\n",
    "out = proc(images=img, text=\"hello\", return_tensors=\"pt\")\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08855131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from typing import Dict,List,Tuple,Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec32a75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir=\"drive_action_output\"\n",
    "chat_file=Path(data_dir).joinpath(\"drive_action_train.json\")\n",
    "chat_data=pd.read_json(path_or_buf=chat_file)\n",
    "chat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3bec434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from show_onevision.data_onevision import LlavaDataset, TrainLLavaOneVisionCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27291773",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llavadataset=LlavaDataset(dataset_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd531539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('View 1: <image>\\nView 2: <image>\\nView 3: <image>\\nThe navigation indicates a left-turn U-turn. As shown in the picture, please determine the types and number of traffic signals ahead in your lane.\\nA. One left-turn arrow signal and one straight circular signal\\nB. Two straight circular signals\\nC. One left-turn arrow signal and two straight circular signals\\nD. Only one straight circular signal.\\nPlease answer with the option letter only (A/B/C/D).',\n",
       " 'C',\n",
       " [PosixPath('drive_action_output/images/1234_image_0.jpg'),\n",
       "  PosixPath('drive_action_output/images/1234_image_1.jpg'),\n",
       "  PosixPath('drive_action_output/images/1234_image_2.jpg')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_llavadataset[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a489fb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('drive_action_output/images/1234_image_0.jpg'),\n",
       " PosixPath('drive_action_output/images/1234_image_1.jpg'),\n",
       " PosixPath('drive_action_output/images/1234_image_2.jpg')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths = test_llavadataset[1234][2]\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0005ba64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only a single or a list of entries is supported but got type=<class 'pathlib.PosixPath'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m out\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/miniconda3/envs/mote/lib/python3.12/site-packages/transformers/models/llava_onevision/processing_llava_onevision.py:165\u001b[0m, in \u001b[0;36mLlavaOnevisionProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m image_inputs \u001b[38;5;241m=\u001b[39m video_inputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     batch_num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(image_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_num_images\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    168\u001b[0m     image_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(image_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/mote/lib/python3.12/site-packages/transformers/image_processing_utils.py:51\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mote/lib/python3.12/site-packages/transformers/models/llava_onevision/image_processing_llava_onevision.py:695\u001b[0m, in \u001b[0;36mLlavaOnevisionImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, image_grid_pinpoints, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, do_convert_rgb, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# only single image patching is supported\u001b[39;00m\n\u001b[1;32m    693\u001b[0m need_patching \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m batch_num_images \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m--> 695\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m images \u001b[38;5;241m=\u001b[39m make_flat_list_of_images(images)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n",
      "File \u001b[0;32m~/miniconda3/envs/mote/lib/python3.12/site-packages/transformers/image_processing_base.py:530\u001b[0m, in \u001b[0;36mImageProcessingMixin.fetch_images\u001b[0;34m(self, image_url_or_urls)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03mConvert a single or a list of urls into the corresponding `PIL.Image` objects.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03mIf a single url is passed, the return value will be a single object. If a list is passed a list of objects is\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03mreturned.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m image_url_or_urls]\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_url_or_urls, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_image(image_url_or_urls)\n",
      "File \u001b[0;32m~/miniconda3/envs/mote/lib/python3.12/site-packages/transformers/image_processing_base.py:536\u001b[0m, in \u001b[0;36mImageProcessingMixin.fetch_images\u001b[0;34m(self, image_url_or_urls)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_url_or_urls\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly a single or a list of entries is supported but got type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image_url_or_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: only a single or a list of entries is supported but got type=<class 'pathlib.PosixPath'>"
     ]
    }
   ],
   "source": [
    "out = proc(images=image_paths, text=\"hello\", return_tensors=\"pt\")\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b795e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e23be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
