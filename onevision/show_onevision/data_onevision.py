from multiprocessing import Value
from typing import Any
from dataclasses import dataclass
import pandas as pd
import torch
from PIL import Image
from torch.utils.data import Dataset
from pathlib import Path
from typing import Dict,List,Tuple,Any,Union
from platform import processor
from transformers import AutoProcessor,LlavaProcessor
from torch.nn.utils.rnn import pad_sequence


class LlavaDataset(Dataset):
    def __init__(self, dataset_dir: str) -> None:
        super().__init__()
        self.chat_data, self.image_dir = self.build_dataset(dataset_dir)

    def build_dataset(self, data_dir: str) -> Tuple[List[Dict], Path]:
        data_dir = Path(data_dir)
        chat_file = data_dir.joinpath("drive_action_train.json")
        image_dir = data_dir.joinpath("images")

        chat_data = pd.read_json(chat_file).to_dict(orient="records")

        return chat_data, image_dir

    def __len__(self):
        return len(self.chat_data)

    def __getitem__(self, index) -> Tuple[str, str, Path]:
        cur_data = self.chat_data[index]
        conversations = cur_data.get("conversations")

        human_input = conversations[0].get("value")
        chatbot_output = conversations[1].get("value")

        # --- ä¿®æ”¹éƒ¨åˆ†ï¼šæ”¯æŒå¤šä¸ªå›¾ç‰‡ ---
        image_list = cur_data.get("image")   # å¯èƒ½æ˜¯ list æˆ– str
        image_paths = []

        if isinstance(image_list, list):
            for img in image_list:
                # img æ˜¯ç±»ä¼¼ "images/10_image_0.jpg"ï¼Œå–æ–‡ä»¶å
                img_path = self.image_dir / Path(img).name
                image_paths.append(img_path)
        else:
            # å•å›¾ç‰‡æƒ…å†µï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            image_paths = [self.image_dir / Path(image_list).name]
        return human_input, chatbot_output, image_paths



@dataclass
class QaImageOutput:
    input_ids: torch.Tensor
    labels: torch.Tensor
    pixel_values: torch.Tensor
    image_sizes: torch.Tensor


def build_qaimage(processor: AutoProcessor, 
                  q_text_from_json: str, 
                  answer: str, 
                  image_paths: List[Union[str, Path]]) -> QaImageOutput:
    
    # 1. System Prompt
    system_prompt = (
        "You are an autonomous driving assistant. "
        "Analyze the multi-view images and answer the user's question accurately."
    )
    
    # 2. æ„é€  User Content (ä¿®æ”¹ä¸ºï¼šæ„é€ çº¯å­—ç¬¦ä¸²)
    # -----------------------------------------------------------
    # [ä¿®æ”¹è¯´æ˜]: Tokenizer æ¨¡æ¿ä¸æ”¯æŒ Listï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ„é€ 
    # åŒ…å« <image> å ä½ç¬¦çš„å­—ç¬¦ä¸²ã€‚
    # -----------------------------------------------------------
    
    # æ¸…æ´—æ–‡æœ¬ (ç§»é™¤ json é‡Œçš„æ—§ <image> å ä½ç¬¦ï¼Œé˜²æ­¢é‡å¤)
    clean_q_text = q_text_from_json.replace("<image>", "").strip()
    
    # æ„é€ å›¾ç‰‡å ä½ç¬¦å­—ç¬¦ä¸²ã€‚
    # å¦‚æœæœ‰ 3 å¼ å›¾ï¼Œè¿™å°±æ˜¯ "<image>\n<image>\n<image>\n"
    # æ³¨æ„ï¼šLLaVA-OneVision é€šå¸¸ä½¿ç”¨ <image> ä½œä¸ºå ä½ç¬¦ï¼ŒProcessor åç»­ä¼šå¤„ç†å®ƒ
    image_tokens_str = ("<image>\n" * len(image_paths))
    
    # æ‹¼æ¥æˆæœ€ç»ˆçš„ç”¨æˆ·è¾“å…¥å­—ç¬¦ä¸²
    user_content_str = image_tokens_str + clean_q_text

    # 3. æ„é€  Messages
    messages = [
        {"role": "system", "content": system_prompt}, 
        # [ä¿®æ”¹è¯´æ˜]: è¿™é‡Œ content ä¼ å…¥ strï¼Œè€Œä¸æ˜¯ list
        {"role": "user",   "content": user_content_str},
    ]

    # 4. ç”Ÿæˆ Prompt æ–‡æœ¬
    # æ­¤æ—¶ä¼ å…¥çš„æ˜¯çº¯æ–‡æœ¬ï¼Œapply_chat_template ä¸ä¼šå†æŠ¥ concatenation é”™è¯¯
    prompt_text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # 5. æ„é€ å®Œæ•´æ–‡æœ¬ (Prompt + Answer + EOS)
    full_text = prompt_text + answer + processor.tokenizer.eos_token

    # 6. åŠ è½½å›¾ç‰‡
    raw_images = [Image.open(Path(p)).convert("RGB") for p in image_paths]

    # 7. Processor ä¸€é”®å¤„ç†
    # LLaVA-OneVision çš„ processor ä¼šæŸ¥æ‰¾ text ä¸­çš„ <image> å­—ç¬¦ä¸²ï¼Œ
    # å¹¶å°†å…¶æ›¿æ¢ä¸ºå¯¹åº”çš„ vision token (å¦‚ <|vision_start|>...<|vision_end|>)
    inputs = processor(
        text=full_text,
        images=raw_images,
        return_tensors="pt",
        padding=True,
        truncation=True
    )

    input_ids = inputs["input_ids"][0]
    pixel_values = inputs["pixel_values"][0] 
    image_sizes = inputs["image_sizes"][0]
    if image_sizes.ndim == 1:
        image_sizes = image_sizes.unsqueeze(0) # ç¡®ä¿å½¢çŠ¶æ˜¯ (Num_Images, 2)
    
    # 8. Mask Labels (SFT é€»è¾‘)
    # è®¡ç®— prompt çš„çœŸå®é•¿åº¦
    prompt_inputs = processor(
        text=prompt_text,
        images=raw_images,
        return_tensors="pt",
        padding=True 
    )
    prompt_len = prompt_inputs["input_ids"].shape[1]

    labels = input_ids.clone()
    # è¿™é‡Œçš„ mask é€»è¾‘ä¿æŒä¸å˜
    if prompt_len < len(labels):
        labels[:prompt_len] = -100
    else:
        labels[:] = -100

    return QaImageOutput(
        input_ids=input_ids,
        labels=labels,
        pixel_values=pixel_values,
        image_sizes=image_sizes
    )






class TrainLLavaOneVisionCollator:
    def __init__(self, processor, IGNORE_INDEX: int = -100):
        self.processor = processor
        self.ignore_index = IGNORE_INDEX
        self.pad_token_id = processor.tokenizer.pad_token_id if processor.tokenizer.pad_token_id is not None else 0

    def __call__(self, features: List) -> Dict[str, torch.Tensor]:
        # --- ğŸ”´ DEBUG START ---
        print(f"\n[DEBUG] Collator called with {len(features)} features")
        try:
            input_ids_list = []
            labels_list = []
            pixel_values_list = []
            image_sizes_list = []

            for i, feature in enumerate(features):
                print(f"[DEBUG] Processing feature {i}...")
                # feature[0]: q_text, feature[1]: answer, feature[2]: paths
                
                # æ‰“å°ä¸€ä¸‹è·¯å¾„çœ‹çœ‹å¯¹ä¸å¯¹
                # print(f"  -> Images: {feature[2]}") 

                qaimage_output = build_qaimage(
                    processor=self.processor,
                    q_text_from_json=feature[0],
                    answer=feature[1],
                    image_paths=feature[2]
                )
                
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸º None
                if qaimage_output is None:
                    print(f"[ERROR] Feature {i} returned None!")
                    continue

                input_ids_list.append(qaimage_output.input_ids)
                labels_list.append(qaimage_output.labels)
                pixel_values_list.append(qaimage_output.pixel_values)
                image_sizes_list.append(qaimage_output.image_sizes)

            print("[DEBUG] Stacking tensors...")
            final_input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.pad_token_id)
            final_labels = pad_sequence(labels_list, batch_first=True, padding_value=self.ignore_index)
            attention_mask = final_input_ids.ne(self.pad_token_id).long()
            final_pixel_values = torch.cat(pixel_values_list, dim=0)
            final_image_sizes = torch.cat(image_sizes_list, dim=0)
            
            #print(f"[DEBUG] Batch prepared. Input shape: {final_input_ids.shape}")
            
            return {
                "input_ids": final_input_ids,
                "labels": final_labels,
                "attention_mask": attention_mask,
                "pixel_values": final_pixel_values,
                "image_sizes": final_image_sizes
            }
        except Exception as e:
            print(f"\n[CRITICAL ERROR] Collator failed: {e}")
            import traceback
            traceback.print_exc()
            raise e
        # --- ğŸ”´ DEBUG END ---
   